[
    {
        "label": "streamlit",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "streamlit",
        "description": "streamlit",
        "detail": "streamlit",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "Redis",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "Redis",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "START",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "MessagesState",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "StateGraph",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "MessagesPlaceholder",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "MemorySaver",
        "importPath": "langgraph.checkpoint.memory",
        "description": "langgraph.checkpoint.memory",
        "isExtraImport": true,
        "detail": "langgraph.checkpoint.memory",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain.text_splitter",
        "description": "langchain.text_splitter",
        "isExtraImport": true,
        "detail": "langchain.text_splitter",
        "documentation": {}
    },
    {
        "label": "DirectoryLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "PyPDFLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "Docx2txtLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "redis",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "redis",
        "description": "redis",
        "detail": "redis",
        "documentation": {}
    },
    {
        "label": "call_model",
        "kind": 2,
        "importPath": "init_chat_model",
        "description": "init_chat_model",
        "peekOfCode": "def call_model(state: MessagesState):\n    user_question = state[\"messages\"][-1].content\n    retrieved_docs = vectorstore.similarity_search_with_score(\n        user_question, k=3)\n    context = \"\\n\\n\".join([doc.page_content for doc, _ in retrieved_docs])\n    final_prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{user_question}\"\n    prompt = prompt_template.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": final_prompt}]\n    })\n    response = model.invoke(prompt)",
        "detail": "init_chat_model",
        "documentation": {}
    },
    {
        "label": "redis_url",
        "kind": 5,
        "importPath": "init_chat_model",
        "description": "init_chat_model",
        "peekOfCode": "redis_url = \"redis://localhost:6379\"\nindex_name = \"my-index\"\nembedding = OllamaEmbeddings(model=\"deepseek-r1:8b\")\nvectorstore = Redis.from_existing_index(\n    embedding=embedding,\n    redis_url=redis_url,\n    index_name=index_name,\n    schema=None\n)\n# üîß LLM i prompt",
        "detail": "init_chat_model",
        "documentation": {}
    },
    {
        "label": "index_name",
        "kind": 5,
        "importPath": "init_chat_model",
        "description": "init_chat_model",
        "peekOfCode": "index_name = \"my-index\"\nembedding = OllamaEmbeddings(model=\"deepseek-r1:8b\")\nvectorstore = Redis.from_existing_index(\n    embedding=embedding,\n    redis_url=redis_url,\n    index_name=index_name,\n    schema=None\n)\n# üîß LLM i prompt\nmodel = ChatOllama(model=\"deepseek-r1:8b\", temperature=0.3)",
        "detail": "init_chat_model",
        "documentation": {}
    },
    {
        "label": "embedding",
        "kind": 5,
        "importPath": "init_chat_model",
        "description": "init_chat_model",
        "peekOfCode": "embedding = OllamaEmbeddings(model=\"deepseek-r1:8b\")\nvectorstore = Redis.from_existing_index(\n    embedding=embedding,\n    redis_url=redis_url,\n    index_name=index_name,\n    schema=None\n)\n# üîß LLM i prompt\nmodel = ChatOllama(model=\"deepseek-r1:8b\", temperature=0.3)\nprompt_template = ChatPromptTemplate.from_messages([",
        "detail": "init_chat_model",
        "documentation": {}
    },
    {
        "label": "vectorstore",
        "kind": 5,
        "importPath": "init_chat_model",
        "description": "init_chat_model",
        "peekOfCode": "vectorstore = Redis.from_existing_index(\n    embedding=embedding,\n    redis_url=redis_url,\n    index_name=index_name,\n    schema=None\n)\n# üîß LLM i prompt\nmodel = ChatOllama(model=\"deepseek-r1:8b\", temperature=0.3)\nprompt_template = ChatPromptTemplate.from_messages([\n    (\"system\", \"\"\"",
        "detail": "init_chat_model",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "init_chat_model",
        "description": "init_chat_model",
        "peekOfCode": "model = ChatOllama(model=\"deepseek-r1:8b\", temperature=0.3)\nprompt_template = ChatPromptTemplate.from_messages([\n    (\"system\", \"\"\"\n    Ti si AI asistent koji poma≈æe s upisom na fakultet pomoƒáu vektoriziranih podataka parsiranih datoteka natjeƒçaja za upise.\n    U sluƒçaju da korisnik pozdravi pozdravi ga nazad i reci ƒçemu slu≈æi≈°.\n    Uvijek odgovaraj jezgrovito i jasno.\n    Ako korisnik pi≈°e na srpskom jeziku, odgovaraj na hrvatskom jeziku.\n    Ako korisnik pi≈°e na nekom drugom jeziku, odgovaraj na istom jeziku.\n    Koristi samo informacije iz konteksta.\n    Odgovori samo ako naƒëe≈° odgovor ne izmi≈°ljaj informacije reci ne znam.",
        "detail": "init_chat_model",
        "documentation": {}
    },
    {
        "label": "prompt_template",
        "kind": 5,
        "importPath": "init_chat_model",
        "description": "init_chat_model",
        "peekOfCode": "prompt_template = ChatPromptTemplate.from_messages([\n    (\"system\", \"\"\"\n    Ti si AI asistent koji poma≈æe s upisom na fakultet pomoƒáu vektoriziranih podataka parsiranih datoteka natjeƒçaja za upise.\n    U sluƒçaju da korisnik pozdravi pozdravi ga nazad i reci ƒçemu slu≈æi≈°.\n    Uvijek odgovaraj jezgrovito i jasno.\n    Ako korisnik pi≈°e na srpskom jeziku, odgovaraj na hrvatskom jeziku.\n    Ako korisnik pi≈°e na nekom drugom jeziku, odgovaraj na istom jeziku.\n    Koristi samo informacije iz konteksta.\n    Odgovori samo ako naƒëe≈° odgovor ne izmi≈°ljaj informacije reci ne znam.\n    \"\"\"),",
        "detail": "init_chat_model",
        "documentation": {}
    },
    {
        "label": "workflow",
        "kind": 5,
        "importPath": "init_chat_model",
        "description": "init_chat_model",
        "peekOfCode": "workflow = StateGraph(state_schema=MessagesState)\nworkflow.add_node(\"model\", call_model)\nworkflow.add_edge(START, \"model\")\nmemory = MemorySaver()\napp = workflow.compile(checkpointer=memory)\n# üåê Streamlit UI\nst.set_page_config(page_title=\"Chat with Uni AI Assistant\", page_icon=\"ü¶ú\")\nst.title(\"AI Asistent za upise na faks\")\n# üß† Init session\nif \"messages\" not in st.session_state:",
        "detail": "init_chat_model",
        "documentation": {}
    },
    {
        "label": "memory",
        "kind": 5,
        "importPath": "init_chat_model",
        "description": "init_chat_model",
        "peekOfCode": "memory = MemorySaver()\napp = workflow.compile(checkpointer=memory)\n# üåê Streamlit UI\nst.set_page_config(page_title=\"Chat with Uni AI Assistant\", page_icon=\"ü¶ú\")\nst.title(\"AI Asistent za upise na faks\")\n# üß† Init session\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n# üí¨ Prika≈æi povijest poruka\nfor msg in st.session_state.messages:",
        "detail": "init_chat_model",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "init_chat_model",
        "description": "init_chat_model",
        "peekOfCode": "app = workflow.compile(checkpointer=memory)\n# üåê Streamlit UI\nst.set_page_config(page_title=\"Chat with Uni AI Assistant\", page_icon=\"ü¶ú\")\nst.title(\"AI Asistent za upise na faks\")\n# üß† Init session\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n# üí¨ Prika≈æi povijest poruka\nfor msg in st.session_state.messages:\n    with st.chat_message(msg[\"role\"]):",
        "detail": "init_chat_model",
        "documentation": {}
    },
    {
        "label": "pdf_loader",
        "kind": 5,
        "importPath": "vectorize_and_save_data",
        "description": "vectorize_and_save_data",
        "peekOfCode": "pdf_loader = DirectoryLoader(\n    path=\"data\", glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\npdf_docs = pdf_loader.load()\ndocx_loader = DirectoryLoader(\n    path=\"data\", glob=\"**/*.docx\", loader_cls=Docx2txtLoader)\nword_docs = docx_loader.load()\nall_docs = pdf_docs + word_docs\nprint(f\"Loaded {len(pdf_docs)} PDF pages.\")\nprint(f\"Loaded {len(word_docs)} Word documents.\")\nprint(f\"Total: {len(all_docs)} documents.\")",
        "detail": "vectorize_and_save_data",
        "documentation": {}
    },
    {
        "label": "pdf_docs",
        "kind": 5,
        "importPath": "vectorize_and_save_data",
        "description": "vectorize_and_save_data",
        "peekOfCode": "pdf_docs = pdf_loader.load()\ndocx_loader = DirectoryLoader(\n    path=\"data\", glob=\"**/*.docx\", loader_cls=Docx2txtLoader)\nword_docs = docx_loader.load()\nall_docs = pdf_docs + word_docs\nprint(f\"Loaded {len(pdf_docs)} PDF pages.\")\nprint(f\"Loaded {len(word_docs)} Word documents.\")\nprint(f\"Total: {len(all_docs)} documents.\")\nembeddings = OllamaEmbeddings(model=\"deepseek-r1:8b\")\nr = redis.Redis(host=\"127.0.0.1\", port=6379)",
        "detail": "vectorize_and_save_data",
        "documentation": {}
    },
    {
        "label": "docx_loader",
        "kind": 5,
        "importPath": "vectorize_and_save_data",
        "description": "vectorize_and_save_data",
        "peekOfCode": "docx_loader = DirectoryLoader(\n    path=\"data\", glob=\"**/*.docx\", loader_cls=Docx2txtLoader)\nword_docs = docx_loader.load()\nall_docs = pdf_docs + word_docs\nprint(f\"Loaded {len(pdf_docs)} PDF pages.\")\nprint(f\"Loaded {len(word_docs)} Word documents.\")\nprint(f\"Total: {len(all_docs)} documents.\")\nembeddings = OllamaEmbeddings(model=\"deepseek-r1:8b\")\nr = redis.Redis(host=\"127.0.0.1\", port=6379)\nr.flushall()",
        "detail": "vectorize_and_save_data",
        "documentation": {}
    },
    {
        "label": "word_docs",
        "kind": 5,
        "importPath": "vectorize_and_save_data",
        "description": "vectorize_and_save_data",
        "peekOfCode": "word_docs = docx_loader.load()\nall_docs = pdf_docs + word_docs\nprint(f\"Loaded {len(pdf_docs)} PDF pages.\")\nprint(f\"Loaded {len(word_docs)} Word documents.\")\nprint(f\"Total: {len(all_docs)} documents.\")\nembeddings = OllamaEmbeddings(model=\"deepseek-r1:8b\")\nr = redis.Redis(host=\"127.0.0.1\", port=6379)\nr.flushall()\nprint(\"Cleared all data from Redis.\")\ntext_splitter = RecursiveCharacterTextSplitter(",
        "detail": "vectorize_and_save_data",
        "documentation": {}
    },
    {
        "label": "all_docs",
        "kind": 5,
        "importPath": "vectorize_and_save_data",
        "description": "vectorize_and_save_data",
        "peekOfCode": "all_docs = pdf_docs + word_docs\nprint(f\"Loaded {len(pdf_docs)} PDF pages.\")\nprint(f\"Loaded {len(word_docs)} Word documents.\")\nprint(f\"Total: {len(all_docs)} documents.\")\nembeddings = OllamaEmbeddings(model=\"deepseek-r1:8b\")\nr = redis.Redis(host=\"127.0.0.1\", port=6379)\nr.flushall()\nprint(\"Cleared all data from Redis.\")\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1500,",
        "detail": "vectorize_and_save_data",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "vectorize_and_save_data",
        "description": "vectorize_and_save_data",
        "peekOfCode": "embeddings = OllamaEmbeddings(model=\"deepseek-r1:8b\")\nr = redis.Redis(host=\"127.0.0.1\", port=6379)\nr.flushall()\nprint(\"Cleared all data from Redis.\")\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1500,\n    chunk_overlap=500,\n)\ndocs_with_splits = text_splitter.split_documents(all_docs)\nprint(f\"Split into {len(docs_with_splits)} document chunks.\")",
        "detail": "vectorize_and_save_data",
        "documentation": {}
    },
    {
        "label": "r",
        "kind": 5,
        "importPath": "vectorize_and_save_data",
        "description": "vectorize_and_save_data",
        "peekOfCode": "r = redis.Redis(host=\"127.0.0.1\", port=6379)\nr.flushall()\nprint(\"Cleared all data from Redis.\")\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1500,\n    chunk_overlap=500,\n)\ndocs_with_splits = text_splitter.split_documents(all_docs)\nprint(f\"Split into {len(docs_with_splits)} document chunks.\")\nvectorstore = Redis.from_documents(",
        "detail": "vectorize_and_save_data",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "vectorize_and_save_data",
        "description": "vectorize_and_save_data",
        "peekOfCode": "text_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1500,\n    chunk_overlap=500,\n)\ndocs_with_splits = text_splitter.split_documents(all_docs)\nprint(f\"Split into {len(docs_with_splits)} document chunks.\")\nvectorstore = Redis.from_documents(\n    docs_with_splits,\n    embeddings,\n    redis_url=\"redis://localhost:6379\",",
        "detail": "vectorize_and_save_data",
        "documentation": {}
    },
    {
        "label": "docs_with_splits",
        "kind": 5,
        "importPath": "vectorize_and_save_data",
        "description": "vectorize_and_save_data",
        "peekOfCode": "docs_with_splits = text_splitter.split_documents(all_docs)\nprint(f\"Split into {len(docs_with_splits)} document chunks.\")\nvectorstore = Redis.from_documents(\n    docs_with_splits,\n    embeddings,\n    redis_url=\"redis://localhost:6379\",\n    index_name=\"my-index\"\n)\nprint(\"Vectors stored in Redis!\")",
        "detail": "vectorize_and_save_data",
        "documentation": {}
    },
    {
        "label": "vectorstore",
        "kind": 5,
        "importPath": "vectorize_and_save_data",
        "description": "vectorize_and_save_data",
        "peekOfCode": "vectorstore = Redis.from_documents(\n    docs_with_splits,\n    embeddings,\n    redis_url=\"redis://localhost:6379\",\n    index_name=\"my-index\"\n)\nprint(\"Vectors stored in Redis!\")",
        "detail": "vectorize_and_save_data",
        "documentation": {}
    }
]